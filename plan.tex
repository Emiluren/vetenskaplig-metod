\documentclass[msc,lith,english]{liuthesis}
\include{settings}

\department{Institutionen för datavetenskap}
\departmentenglish{Department of Computer and Information Science}
\departmentshort{IDA}

\supervisor{Min handledare}
\examiner{Min examinator}
\titleenglish{Analysing applicability and usefulness of Grover Search for common NP-complete problems}
%\subtitleenglish{}
\thesissubject{Datateknik}

\publicationyear{19}
\currentyearthesisnumber{001}
\dateofpublication{2019-05-08}
\addbibresource{sources.bib}

\author{\parbox{\textwidth}{Emil Segerbäck\\ Olav Övrebö}}

\begin{document}

\chapter{Problem Description}
\section{Background}
NP (non-deterministic Polynomial) denotes the class of decision problems solvable in polynomial time given a non-deterministic Turing machine (NDTM). These come paired with a functional problem, forming the class FNP (functional non-deterministic), with the NP problem typically asking whether its FNP equivalent has a solution. Analogous to solving a problem with an NDTM, an NP-problem's solution may be validated in polynomial time on a deterministic Turing machine such as a classical computer, given the paired FNP-solution. Furthermore, a problem is described as NP-hard if it can be shown that any problem in NP can be reduced into it, with at most a polynomial change in problem size, in polynomial-bounded time. Importantly, NP-hard problems which themselves belong to NP, called NP-complete, hold both these traits, being verifiable in polynomial time on a classical computer and presenting a worst-case computational difficulty for the whole of NP \cite{CCAMA}.

Grover’s Search algorithm is a quantum algorithm which solves SAT-problems (boolean constraint satisfaction problems). It also solves their FNP equivalent that is, producing a set of boolean values which satisfy the constraints, if at all possible, with a quadratic reduction in complexity relative iterating over proposed FNP-solutions and attempting to validate these \cite{QCQI}\cite{CCAMA}. The most encompassing of SAT-problems are NP-complete. The “search” phrasing relates to the applicability in searching lists already converted into a quantum circuit, which it achieves in $\mathcal{O}(\sqrt{N})$ time for $N$ elements. In brevity this is achieved by repeated application of a circuit based on the relevant validation algorithm, shifting the potential value space of an array of superposition qbits into the set of outcomes that fulfil the validation, if the set is non-empty. This is a variation of what is called Amplitude Amplification. While this polynomial speed-up is not enough to provide NP-problems a solution in polynomial time, it may as much as double the computable input size from what is achievable in classical computing \cite{EIQC}.

In this thesis the applicability of Grover's Search in solving a wide variety of NP-complete problems is addressed. In particular, the aim is to roughly map out the achievable growth in problem size during reduction to SAT-format for various problems representative of different categories of problems. There are several benefits to bettering the understanding of such growth, and relations between different NP-complete problems. They chiefly pertain to improving the calculability of otherwise difficult-to-approach problems by as much as doubling the input sizes for which they are solvable, given future advances in construction of quantum computers; potentially the only such option if both BQP (quantum-computer polynomial) and P (deterministic Turing machine-polynomial) complexity classes are separate from NP, as is a widely held but unproven belief. There is also a hope to better the understanding of interrelations between problems, particularly within the NP-complete set. Lastly, the project aims to increase interest in quantum computation and quantum algorithms, by presenting uses outside of solving BQP problems, should a delimitation on the class ever be proven.

\section{Research questions}
The thesis tackles the following questions/tasks:
\begin{itemize}
    \item For the problems treated, what worst-case growth in size can be established during reduction into a Grover-searchable format?
    \item For which of these is the growth sufficiently small that applying Grover's Search to them produces a useful speedup relative both to best-known classical algorithms, and to the trivial solution?
    \item Can a lower-bound in growth be proven for any problems where a sufficient reduction is not successfully produced, proving such an approach pointless?
\end{itemize}

\chapter{Theory}

\section{Quantum Computation}

\subsection{Concept}
The idea of mapping information processing tasks onto quantum dynamical phenomenons have been around since the 1970:s \cite{QCQI}. Fundamentally, such computation diverges from classical, deterministic Turing machines. Principally, they have more in common with the theoretical models of probabilistic Turing machines which are non-deterministic, but do not offer the 'perfect choice' of NDTM:s \cite{CCAMA}. However, two particular properties of quantum dynamical systems make even such models inapplicable: superposition and entanglement. 

The former, superposition of a quantum particle (or qbit, in quantum information theory), relates to the probabilistic aspects of quantum computers. It is the idea that a particle, along some chosen basis of measurement, can only be described by the probabilities of it being measured as 1 or 0. More specifically, they must be represented by a complex-valued state, from which the real-valued probabilities can be derived. Commonly, this state is represented as a point on the surface of a sphere called a Bloch sphere, where the point's projection on the y-axis (of the chosen basis of measurement) gives the probability of being measured as a 1 or 0. Quantum dynamics then allow for any operation that performs a reversible change in state, but disallows any irreversible operations, according to the empirical findings of physicists. This leads to, amongst other things, the inability to clone the state from one qbit to another and maintain the first. Importantly, measurement collapses all probabilities for the states into a singular outcome, at either 1 or 0 along the chosen y-axis, destroying any information and so being the one exception to the rule of 'no irreversible state change' \cite{QCQI}.

The latter concept, entanglement, makes the measurements of two quantum bits interdependent to the point where they can no longer be treated as separate entities, but rather as a single qbit-array, with a state describing probabilities for any of the $2^N$ measurable states of $N$ entangled qbits. For example, two entangled qbits might have a $\frak{1}{3}$ probability each of being measured as 00, 01 or 10, but be impossible to measure as 11, making it impossible to describe the state as a Cartesian product of separate outcome probabilities for the different bits. The fact that $2^N$ states are all simultaneously operated on by a single quantum dynamical operation (or quantum gate, as they are referred to) gives rise to a potentially exponential growth in computational power per qbit, assuming the problem to be solved can be sufficiently well expressed and resolved with the limitations on operations imposed by quantum dynamics \cite{QCQI}\cite{EIQC}. 

\subsection{Implementation}
In theory, quantum computation can be performed on any quantum dynamical system on implementation is feasible for both both a two-qbit gate (generally CNOT, see later) and arbitrary rotation of the state of singular qbits. k-qbit gates are gates which operate on one or more out of k qbits, dependent on the state of several of these. Examples include SWAP-gates (where, for each possible outcome, two bits swap value completely), and CNOT, where a qbit is 'flipped' based on the value of another. Obviously, these lead to a rearrangement of several (possibly all) of the probabilities of each measurement outcome\cite{QCQI}. Provided these possibilities, the system is quantum computationally complete, analogous to how any classical logical operation may be constructed from NAND-gates alone. These quantum dynamical systems may use particles, energy or something in between (quasi-particles), though different implementations offer different advantages and disadvantages. A few common candidates include nuclear magnetic radiation (NMR), the polarity of photons and ion traps (ions caught in a line, held between a set of repelling rods and having different spins induced by careful application of a laser) \cite{QCQI}\cite{EIQC}. There are also yet more being researched, such as the topological quantum computer (locking and entangling anyons in a two-dimensional electron gas, a common physical model of solid state physics). This last example interestingly promises to be highly error-resistant, whereas current technologies are highly susceptible to noise, even in the near absolute zero temperatures that they typically operate \cite{EIQC}. 

Common drawbacks include noise, cost to produce/maintain, difficulty in accurately enacting two-qbit gates between physically distant qbits (as is especially the case with the ion trap) and difficulty to measure qbits, amongst other things. In regards to interactivity between qbits, one of the more useful approaches appears to be the use of NMR, which is thus amongst the most popular implementations \cite{QCQI}.

\section{Prior research}
Under this section a number of research papers are collected and discussed in relation to this thesis. These have been selected because they either deal with reduction of problems into SAT, as is appropriate for this thesis, or because they somehow apply Grover Search to provide a speedup over classical algorithms, similar to what the aim of this project but with narrower focus and greater depth.

\subsection{Quantum Search and SAT-solving}
Boolean satisfiability problems, where it is decided whether or not a Boolean formula is counter-tautological or not, are an important and common problem in computer science, the development of algorithms for which is therefore of great interest. In classical computation, the DPLL algorithm, a re-working of the prior DP algorithm, is a common approach to determining such traits of logical formulas in, or rewritten to, conjunctive normal form (CNF) \cite{QWSBA}. Typically, all possible value assignments must be shown not to satisfy the formula for it to be proven counter-tautological. As such, the DPLL approach, presented in the 1962 paper ``A machine program for theorem proving'' is to perform a depth-first-search across all possible assignments, but not exploring in deapth any sub-graph of the search tree that can through simple resolution be shown counter-tautological. This, obviously, produces a worst case time complexity of $\mathcal{O(2^n)}$ for $n$ variable formulas, as there are $2^n$ such Boolean value-assignments to such a formula \cite{DPLL}.  

In his 1997 paper Lov K. Grover presents his approach to Quantum Search, both on a conceptual and a practical level, detailing the algorithm and providing aid in visualising the operation via analogues to geometrical operations. The algorithm, as he puts it, requires fundamentally simple circuitry compared to many other quantum algorithms, and provides a noteworthy improvement over classical search. His algorithm fundamentally creates an equal-probability outcome superposition of entangled qbits, and amplifies the probability of a desired outcome by process of repeated phase-shift of desired outcome probabilities, termed an Oracle Transformation, and inversion about their average amplitude. Lastly, it is shown precisely how the amplitude amplification scales with input, $\mathcal{O}(\frac{1}{\sqrt{N}})$ for a search-space of size $N$, from which the computational complexity of the search algorithm, $\mathcal{O}(\sqrt{N})$, follows \cite{QMHSNH}.

Relatedly, Avatar Tulsi has done some work on an alternative search algorithm to the selective phase inversion of the solution state in Grover's algorithm. The algorithm is specifically tailored to clause satisfaction problems and relaxes the requirement of coupling between computation circuits for each binary function that is part of the clause. Because of this it has a potential to save computational resources required to implement the oracle transformation in Grover's search \cite{QuantumCSPSearch}.

%\subsection{Translation of Algorithmic Descriptions of Discrete Functions to SAT with Applications to Cryptoanalysis Problems}
%Transalg is a software system meant to be used for SAT-based cryptoanalysis. Methods to translate algorithmic descriptions of discrete functions to SAT are used to implement transalg \cite{Transalg}.

Lastly, Ashley Montanaro engages in reworking and refining quantum walk algorithms, based on nested Grover Search across partially revealed search spaces in search trees. This is presented as a polynomial speedup in solving chiefly CNF CSP:s and general graph search problems, by integration into classical backtracking algorithms like the DPLL algorithm mentioned above. Particularly, the paper also specifies relaxed cases where an exponential speed-up is achievable over classical algorithms in the average case \cite{QWSBA}.

\subsection{On performing research producing algorithmic reductions to SAT}

Algorithmic reduction from one problem description to another in polynomial time, more properly called Karp reduction or many-to-one reduction \cite{CCAMA}, typically requires some ad-hoc approach to the particular problem-problem pair being approached. Hence, a significant portion of the work cannot itself be approached through some standardised method, and will require significant analysis and trial-and-error before an actual solution method can be established a priori. However, a useful initial method for many problems to-be-reduced approached during the thesis work will be the recent work by Predrag Janicic in developing a problem-expression language, URSA, and tools for non-general but widely encompassing uniform reduction of problems expressed in said language to SAT-form \cite{URSA}. Following the application of such tools, where applicable, the performance of the resulting reduction will need to be analysed and potentially reworked or replaced with a non-general approach, as appropriate for the problem in question. 

To then ensure legibility of our produced reductions and subsequent proofs of their related problem size growth rates some redundancy in mathematical phrasing will be necessary, with methods and proofs being expressed both conceptually and formally so as to better convey the results. This, along with proper structure and nomenclature, should aid any reader in absorbing the material, in accordance with Halmos' essay ``How to Write Mathematics'' \cite{HTWM}.

\chapter{Method}
Firstly, a delimitation to a reasonable number of target problems must be established, proposedly via discussion with the assigned supervisor. A selection must then be made as to which problems specifically to target. The initial approach will be to aim for a set of representative problems first and foremost. Inclusion of different variations and relaxations of the same problem, such as testing for finding both Hamilton paths and cycles, will be avoided. Only a particular class or type of problem tested should yield particular results, further problems within the class may be tested as well. For potentially proving any lower bounds, various approaches will be used as is deemed appropriate for the relevant reduction. In doing so, the aim will be to show a growth in problem input size by more than a factor of two, as at that point the Grover Search algorithm no longer offers any improvement over a classical computation of the problem. It is worth noting that any NP-hard problem approached will be solvable by the proposed method, since all problems in NP can be reduced to any NP-complete problem, thus also to SAT-problems to which Grover’s algorithm can be applied. Furthermore, no actual quantum computation or simulation need be performed for the sake of the thesis; the work to be done is bounded to classical problem reduction and analysis.

\chapter{Time plan}
This chapter contains an estimated plan for the thesis. There is a short description of each activity and a risk analysis of possibly time consuming events.

\section{Activities}
The following activities are shown in Table~\ref{gantt}.

\subsection{Planning}
Planning the thesis work. This is mostly made up of writing this document.

\subsection{Select problems}
A reasonable number of target problems will be selected together with the supervisor.

\subsection{Find worst case growth}
Find the worst case growth in size for the selected problems during reduction into a Grover-searchable format.

\subsection{Determine useful speedup}
Determine which of the selected problems the growth i sufficiently small that applying Grover's search produces a useful speedup.

\subsection{Examine failed reductions}
Attempt to find a lower bound on the growth for any problems where a sufficient reduction could not be found.

\begin{table}
  \begin{ganttchart}[hgrid, vgrid]{1}{23}
    \gantttitle{Time plan Gantt chart}{23} \\
    \gantttitlelist{1,...,23}{1} \\
    \ganttbar{Planning}{3}{4} \\
    \ganttbar{Study Grover's search}{5}{5} \\
    \ganttbar{Select problems}{5}{7} \\
    \ganttbar{Find worst-case growth}{8}{10} \\
    \ganttbar{Determine useful speedup}{11}{12} \\

    \ganttmilestone{Half-time thesis review}{12} \\

    \ganttbar{Examine failed reductions}{13}{14} \\

    \ganttbar{Report: Introduction}{7}{7} \\
    \ganttbar{Report: Theory}{10}{12} \\
    \ganttbar{Report: Method}{13}{16} \\
    \ganttbar{Report: Results}{17}{18} \\
    \ganttbar{Report: Discussion}{19}{20} \\
    \ganttbar{Report: Finalise}{21}{22} \\
    \ganttbar{Opponering}{23}{23}
  \end{ganttchart}
  \caption{Gantt chart of the time plan}\label{gantt}
\end{table}

\section{Risk analysis}
There is always a risk of things not going exactly as planned. The following subsections will try to anticipate some of the possible problems that might appear during the project.

\subsection{Difficult theory}
One of the people in the group is not experienced in the area of quantum computers and so there is a very real risk that the required theory might prove to be too complex for this project member. That would mean that the other member might end up with a majority of the workload.

\subsection{No suitable reductions found}
It could be the case that no suitable reductions are found for the attempted problems. This could perhaps change the focus of the work slightly to why these speedups were hard to find.

\appendix
\chapter{Changes}
\subsection{Seminar 3 changes}
\begin{itemize}
    \item Sentences have been simplified for legibility.
    \item The section ``Related Work'' has been added.
    \item Explanations of various technical terms have been clarified or expanded upon.
    \item Parts of the introduction have been rewritten and restructured to better engage the reader.
    \item Research questions have been rephrased for clarity, and narrowed down for a clearer scope.
    \item Citations have been added.
    \item Chapter ``Initial approach'' has been renamed Method, and is subject to further change later into the project.
\end{itemize}

\subsection{Seminar 5 changes}
\begin{itemize}
    \item A section familiarising the reader with some fundamental concepts in quantum information processing has been added to the theory chapter.
    \item Related work section has been restructured to better and partially rewritten to better fit with the overall text.
    \item Section on the Transalg algorithm has been removed from Related Work, and replaced with a closer look at classical approaches to SAT-solving.
    \item Text has been re-written to reduce or remove references to the authors.
    \item A redundant research question, of interest only prior to the project, has been removed.
    \item Incomplete citation in bibliography has been expanded upon.
    \item Various phrases have been clarified.
    \item Improper sentences have been restructured or removed.
    \item A section on various stages of work has been introduced, along with a risk analysis for the most critical of these.
    \item Gantt-chart detailing the time plan of the project has been added.
\end{itemize}

\printbibliography
\end{document}